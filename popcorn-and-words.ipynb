{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unlabeledTrainData.tsv', 'labeledTrainData.tsv', 'testData.tsv', 'sampleSubmission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#train and test data\n",
    "train = pd.read_csv('../input/labeledTrainData.tsv', delimiter = '\\t', encoding = 'utf-8')\n",
    "test = pd.read_csv('../input/testData.tsv', delimiter = '\\t', encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews \n",
    "review = train['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing html tags\n",
    "\n",
    "review_text = [re.sub(r'\\<.*\\>', \"\", i) for i in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuations\n",
    "filter_punc = RegexpTokenizer(r'\\w+')\n",
    "list_filter = [filter_punc.tokenize(i) for i in review_text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert everything to lower case\n",
    "review_lower = []\n",
    "for words in list_filter:\n",
    "    lower_case = [i.lower() for i in words]\n",
    "    review_lower.append(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove StopWords!\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = []\n",
    "for words in review_lower:\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words and not word.isdigit():\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            clean_words.append(word)\n",
    "    text.append(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_review = []\n",
    "for words in text:\n",
    "    string = \"\"\n",
    "    for word in words:\n",
    "        string = string + \" \" + word\n",
    "    string_review.append(string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(string_review)\n",
    "list_seq = tokenizer.texts_to_sequences(string_review)\n",
    "X = pad_sequences(list_seq, maxlen=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 53s 3ms/step - loss: 0.4719 - acc: 0.7688 - val_loss: 0.3637 - val_acc: 0.8436\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.2830 - acc: 0.8822 - val_loss: 0.3748 - val_acc: 0.8414\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.2172 - acc: 0.9170 - val_loss: 0.4136 - val_acc: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43d834eb38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM, Bidirectional, GlobalMaxPool1D, Embedding, Dropout\n",
    "from keras.models import Sequential \n",
    "\n",
    "max_feat = 6000\n",
    "embd_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_feat,embd_size ))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ReviewPredictor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning test reviews\n",
    "reviews_y = test['review']\n",
    "#removing HTML Tags\n",
    "review_text_y = [re.sub(r'\\<.*\\>', \"\", i) for i in reviews_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "list_filter_y = [filter_punc.tokenize(i) for i in review_text_y]\n",
    "#convert everything to lower case\n",
    "review_lower_y = []\n",
    "for words in list_filter_y:\n",
    "    lower_case_y = [i.lower() for i in words]\n",
    "    review_lower_y.append(lower_case_y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "text = []\n",
    "for words in review_lower_y:\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words and not word.isdigit():\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            clean_words.append(word)\n",
    "    text.append(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion to string\n",
    "string_review = []\n",
    "for words in text:\n",
    "    string = \"\"\n",
    "    for word in words:\n",
    "        string = string + \" \" + word\n",
    "    string_review.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_test = tokenizer.texts_to_sequences(string_review)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "y_pred = (prediction > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = [int(i) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = {'id':test['id'], 'sentiment':y_predictions}\n",
    "submission = pd.DataFrame(dataframe)\n",
    "submission.to_csv('submissionData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
